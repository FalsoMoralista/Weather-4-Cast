/home/lucianodourado/micromamba/envs/dinov3/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/lucianodourado/micromamba/envs/dinov3/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:root:called-params configs/plantnet300k.yaml
INFO:root:loaded params...
{   'data': {   'batch_size': 1,
                'cache_path': '../cache/plantnet_300k',
                'color_jitter_strength': 0.0,
                'crop_scale': [0.3, 1.0],
                'crop_size': 224,
                'cutmix': 0.0,
                'drop_path': 0.25,
                'image_folder': '/home/rtcalumby/adam/luciano/plantnet_300K/',
                'mixup': 0.0,
                'nb_classes': 1081,
                'num_workers': 16,
                'pin_mem': True,
                'reprob': 0.25,
                'resume_epoch': 0,
                'root_path': '/home/rtcalumby/adam/luciano/',
                'use_color_distortion': False,
                'use_gaussian_blur': False,
                'use_horizontal_flip': False},
    'dinov2': False,
    'dinov2_meta': {'model_name': 'vit_large'},
    'k_means': {'K_range': [2, 3, 4, 5], 'reinitialize_centroids': 5},
    'logging': {   'folder': '/home/rtcalumby/adam/luciano/LifeCLEFPlant2022/logs/PlantNet300k_exp80',
                   'write_tag': 'jepa'},
    'mask': {   'allow_overlap': False,
                'aspect_ratio': [0.75, 1.5],
                'enc_mask_scale': [0.85, 1.0],
                'min_keep': 10,
                'num_enc_masks': 1,
                'num_pred_masks': 4,
                'patch_size': 14,
                'pred_mask_scale': [0.15, 0.2]},
    'meta': {   'copy_data': False,
                'load_checkpoint': True,
                'model_name': 'vit_huge',
                'pred_depth': 12,
                'pred_emb_dim': 384,
                'read_checkpoint': None,
                'use_bfloat16': True},
    'optimization': {   'ema': [0.9, 0.999],
                        'epochs': 100,
                        'final_lr': 7.5e-05,
                        'final_weight_decay': 0.4,
                        'ipe_scale': 1.0,
                        'label_smoothing': 0.1,
                        'lr': 0.001,
                        'start_lr': 1e-06,
                        'warmup': 20,
                        'weight_decay': 0.05},
    'vicreg': {'alpha': 0, 'beta': 0.0, 'gamma': 0.0}}
INFO:root:Running... (rank: 0/1)
INFO:root:Initialized (rank/world-size) 0/1
INFO:root:Sat dataset created
INFO:root:Sat Dataset dataloader created
INFO:root:Sat dataset created
INFO:root:Sat Dataset dataloader created
Training dataset, length: 314664
V-jepa Total parameters: 1.611046912 B
INFO:dinov3:using base=100 for rope new
INFO:dinov3:using min_period=None for rope new
INFO:dinov3:using max_period=None for rope new
INFO:dinov3:using normalize_coords=separate for rope new
INFO:dinov3:using shift_coords=None for rope new
INFO:dinov3:using rescale_coords=2 for rope new
INFO:dinov3:using jitter_coords=None for rope new
INFO:dinov3:using dtype=fp32 for rope new
INFO:dinov3:using swiglu64 layer as FFN
Dinov3 Model: DinoVisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 4096, kernel_size=(16, 16), stride=(16, 16))
    (norm): Identity()
  )
  (rope_embed): RopePositionEmbedding()
  (blocks): ModuleList(
    (0-39): 40 x SelfAttentionBlock(
      (norm1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
      (attn): SelfAttention(
        (qkv): LinearKMaskedBias(in_features=4096, out_features=12288, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=4096, out_features=4096, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): LayerScale()
      (norm2): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
      (mlp): SwiGLUFFN(
        (w1): Linear(in_features=4096, out_features=8192, bias=True)
        (w2): Linear(in_features=4096, out_features=8192, bias=True)
        (w3): Linear(in_features=8192, out_features=4096, bias=True)
      )
      (ls2): LayerScale()
    )
  )
  (norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (local_cls_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (head): Identity()
)
Model Total parameters: 1.611046948 B
allocated mem from model setup: 18.511224269866943
Batch Size: 1
INFO:root:DistributedDataParallel(
  (module): ModelWrapper(
    (backbone): DinoVisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 4096, kernel_size=(16, 16), stride=(16, 16))
        (norm): Identity()
      )
      (rope_embed): RopePositionEmbedding()
      (blocks): ModuleList(
        (0-39): 40 x SelfAttentionBlock(
          (norm1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
          (attn): SelfAttention(
            (qkv): LinearKMaskedBias(in_features=4096, out_features=12288, bias=False)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=4096, out_features=4096, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): LayerScale()
          (norm2): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
          (mlp): SwiGLUFFN(
            (w1): Linear(in_features=4096, out_features=8192, bias=True)
            (w2): Linear(in_features=4096, out_features=8192, bias=True)
            (w3): Linear(in_features=8192, out_features=4096, bias=True)
          )
          (ls2): LayerScale()
        )
      )
      (norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
      (local_cls_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
      (head): Identity()
    )
    (vjepa): VisionTransformer(
      (patch_embed): Identity()
      (blocks): ModuleList(
        (0-7): 8 x Block(
          (norm1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
          (attn): RoPEAttention(
            (qkv): Linear(in_features=4096, out_features=12288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=4096, out_features=4096, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=4096, out_features=16384, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=16384, out_features=4096, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
    )
    (downsample): Conv2d(11, 3, kernel_size=(1, 1), stride=(1, 1))
    (normalize): Normalize(mean=[0.43, 0.411, 0.296], std=[0.213, 0.156, 0.143])
  )
)
INFO:root:Epoch 1
/home/lucianodourado/micromamba/envs/dinov3/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/lucianodourado/micromamba/envs/dinov3/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/lucianodourado/micromamba/envs/dinov3/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/lucianodourado/micromamba/envs/dinov3/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/lucianodourado/micromamba/envs/dinov3/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/lucianodourado/micromamba/envs/dinov3/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/lucianodourado/micromamba/envs/dinov3/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/lucianodourado/micromamba/envs/dinov3/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/lucianodourado/micromamba/envs/dinov3/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/lucianodourado/micromamba/envs/dinov3/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/lucianodourado/micromamba/envs/dinov3/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/lucianodourado/micromamba/envs/dinov3/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/lucianodourado/micromamba/envs/dinov3/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/lucianodourado/micromamba/envs/dinov3/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/lucianodourado/micromamba/envs/dinov3/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/lucianodourado/micromamba/envs/dinov3/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/lucianodourado/micromamba/envs/dinov3/lib/python3.11/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/lucianodourado/weather-4-cast/energy_based_dinov3.py:375: UserWarning: Using a target size (torch.Size([1, 16, 1, 252, 252])) that is different to the input size (torch.Size([1, 784, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  loss = F.smooth_l1_loss(vjepa_embeddings,y)
Process Process-1:
Traceback (most recent call last):
  File "/home/lucianodourado/micromamba/envs/dinov3/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/lucianodourado/micromamba/envs/dinov3/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/lucianodourado/weather-4-cast/main_dinov3.py", line 54, in process_main
    app_main(args=params)
  File "/home/lucianodourado/weather-4-cast/energy_based_dinov3.py", line 415, in main
    (loss, _new_lr, _new_wd), etime = (gpu_timer(train_step))
                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/lucianodourado/weather-4-cast/src/utils/logging.py", line 24, in gpu_timer
    result = closure()
             ^^^^^^^^^
  File "/home/lucianodourado/weather-4-cast/energy_based_dinov3.py", line 375, in train_step
    loss = F.smooth_l1_loss(vjepa_embeddings,y)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lucianodourado/micromamba/envs/dinov3/lib/python3.11/site-packages/torch/nn/functional.py", line 3684, in smooth_l1_loss
    expanded_input, expanded_target = torch.broadcast_tensors(input, target)
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lucianodourado/micromamba/envs/dinov3/lib/python3.11/site-packages/torch/functional.py", line 77, in broadcast_tensors
    return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: The size of tensor a (4096) must match the size of tensor b (252) at non-singleton dimension 4
X: torch.Size([1, 4, 11, 224, 224])
y: torch.Size([1, 16, 1, 252, 252])
Max allocated mem from feature extract: 28.594929218292236
[rank0]:[W827 19:07:05.963763065 ProcessGroupNCCL.cpp:1479] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
