/home/lucianodourado/micromamba/envs/dinov3/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/lucianodourado/micromamba/envs/dinov3/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
[INFO    ][2025-08-21 23:20:48][root                ][process_main             ] called-params configs/plantnet300k.yaml
[INFO    ][2025-08-21 23:20:48][root                ][process_main             ] loaded params...
[W821 23:20:48.178540079 CUDAFunctions.cpp:109] Warning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11080). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (function operator())
{   'data': {   'batch_size': 16,
                'cache_path': '../cache/plantnet_300k',
                'color_jitter_strength': 0.0,
                'crop_scale': [0.3, 1.0],
                'crop_size': 224,
                'cutmix': 0.0,
                'drop_path': 0.25,
                'image_folder': '/home/rtcalumby/adam/luciano/plantnet_300K/',
                'mixup': 0.0,
                'nb_classes': 1081,
                'num_workers': 16,
                'pin_mem': True,
                'reprob': 0.25,
                'resume_epoch': 0,
                'root_path': '/home/rtcalumby/adam/luciano/',
                'use_color_distortion': False,
                'use_gaussian_blur': False,
                'use_horizontal_flip': False},
    'dinov2': False,
    'dinov2_meta': {'model_name': 'vit_large'},
    'k_means': {'K_range': [2, 3, 4, 5], 'reinitialize_centroids': 5},
    'logging': {   'folder': '/home/rtcalumby/adam/luciano/LifeCLEFPlant2022/logs/PlantNet300k_exp80',
                   'write_tag': 'jepa'},
    'mask': {   'allow_overlap': False,
                'aspect_ratio': [0.75, 1.5],
                'enc_mask_scale': [0.85, 1.0],
                'min_keep': 10,
                'num_enc_masks': 1,
                'num_pred_masks': 4,
                'patch_size': 14,
                'pred_mask_scale': [0.15, 0.2]},
    'meta': {   'copy_data': False,
                'load_checkpoint': True,
                'model_name': 'vit_huge',
                'pred_depth': 12,
                'pred_emb_dim': 384,
                'read_checkpoint': None,
                'use_bfloat16': True},
    'optimization': {   'ema': [0.9, 0.999],
                        'epochs': 100,
                        'final_lr': 7.5e-05,
                        'final_weight_decay': 0.4,
                        'ipe_scale': 1.0,
                        'label_smoothing': 0.1,
                        'lr': 0.001,
                        'start_lr': 1e-06,
                        'warmup': 20,
                        'weight_decay': 0.05},
    'vicreg': {'alpha': 0, 'beta': 0.0, 'gamma': 0.0}}
[INFO    ][2025-08-21 23:20:48][root                ][init_distributed         ] Rank: 0. Distributed training not available ProcessGroupNCCL is only supported with GPUs, no GPUs found!
[INFO    ][2025-08-21 23:20:48][root                ][process_main             ] Running... (rank: 0/1)
Cuda not available
[INFO    ][2025-08-21 23:20:48][root                ][init_distributed         ] SLURM vars not set (distributed training not available)
[INFO    ][2025-08-21 23:20:48][root                ][main                     ] Initialized (rank/world-size) 0/1
[INFO    ][2025-08-21 23:20:48][dinov3              ][__init__                 ] using base=100 for rope new
[INFO    ][2025-08-21 23:20:48][dinov3              ][__init__                 ] using min_period=None for rope new
[INFO    ][2025-08-21 23:20:48][dinov3              ][__init__                 ] using max_period=None for rope new
[INFO    ][2025-08-21 23:20:48][dinov3              ][__init__                 ] using normalize_coords=separate for rope new
[INFO    ][2025-08-21 23:20:48][dinov3              ][__init__                 ] using shift_coords=None for rope new
[INFO    ][2025-08-21 23:20:48][dinov3              ][__init__                 ] using rescale_coords=2 for rope new
[INFO    ][2025-08-21 23:20:48][dinov3              ][__init__                 ] using jitter_coords=None for rope new
[INFO    ][2025-08-21 23:20:48][dinov3              ][__init__                 ] using dtype=fp32 for rope new
[INFO    ][2025-08-21 23:20:48][dinov3              ][__init__                 ] using swiglu64 layer as FFN
Process Process-1:
Traceback (most recent call last):
  File "/home/lucianodourado/micromamba/envs/dinov3/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/lucianodourado/micromamba/envs/dinov3/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/lucianodourado/weather-4-cast/main_dinov3.py", line 54, in process_main
    app_main(args=params)
  File "/home/lucianodourado/weather-4-cast/energy_based_dinov3.py", line 222, in main
    print('allocated mem from DinoV3 loading:', allocated_gb)
                                                ^^^^^^^^^^^^
UnboundLocalError: cannot access local variable 'allocated_gb' where it is not associated with a value
Dinov3 Model: DinoVisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 4096, kernel_size=(16, 16), stride=(16, 16))
    (norm): Identity()
  )
  (rope_embed): RopePositionEmbedding()
  (blocks): ModuleList(
    (0-39): 40 x SelfAttentionBlock(
      (norm1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
      (attn): SelfAttention(
        (qkv): LinearKMaskedBias(in_features=4096, out_features=12288, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=4096, out_features=4096, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): LayerScale()
      (norm2): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
      (mlp): SwiGLUFFN(
        (w1): Linear(in_features=4096, out_features=8192, bias=True)
        (w2): Linear(in_features=4096, out_features=8192, bias=True)
        (w3): Linear(in_features=8192, out_features=4096, bias=True)
      )
      (ls2): LayerScale()
    )
  )
  (norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (local_cls_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (head): Identity()
)
